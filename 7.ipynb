{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "749b9ac3",
   "metadata": {},
   "source": [
    "Regularized linear models are a key technique in machine learning for preventing overfitting. Overfitting occurs when a model learns not only the underlying pattern in the data but also the noise, resulting in poor generalization to new, unseen data. Regularization addresses this by adding a penalty to the model complexity, discouraging overly complex models.\n",
    "\n",
    "Types of Regularization in Linear Models:\n",
    "Ridge Regression (L2 Regularization):\n",
    "\n",
    "Adds a penalty equal to the square of the magnitude of coefficients.\n",
    "The regularization term is the sum of the squares of all feature weights: \n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "�\n",
    "�\n",
    "2\n",
    "λ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " w \n",
    "i\n",
    "2\n",
    "​\n",
    " , where \n",
    "�\n",
    "λ is the regularization parameter.\n",
    "It shrinks the coefficients but does not set any to zero, which means it does not perform feature selection.\n",
    "Lasso Regression (L1 Regularization):\n",
    "\n",
    "Adds a penalty equal to the absolute value of the magnitude of coefficients.\n",
    "The regularization term is the sum of the absolute values of all feature weights: \n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∣\n",
    "�\n",
    "�\n",
    "∣\n",
    "λ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " ∣w \n",
    "i\n",
    "​\n",
    " ∣.\n",
    "Tends to produce sparse models (with few coefficients), effectively performing feature selection.\n",
    "Elastic Net:\n",
    "\n",
    "A combination of L1 and L2 regularization.\n",
    "It has two parameters to control the mix of L1 and L2 penalties, providing a balance between Ridge and Lasso.\n",
    "How Regularization Prevents Overfitting:\n",
    "Shrinking Coefficients: Regularization techniques shrink the coefficients towards zero, which decreases model complexity and helps to reduce the risk of overfitting.\n",
    "\n",
    "Penalizing Large Coefficients: Large coefficients can cause a model to be sensitive to small changes in input features, leading to overfitting. Regularization penalizes these large coefficients.\n",
    "\n",
    "Trade-off Between Bias and Variance: Regularization introduces a bit more bias into the model to have a significant drop in variance, leading to better generalization.\n",
    "\n",
    "Feature Selection (Lasso): By reducing some coefficients to zero, Lasso can help in feature selection, which can improve model interpretability and reduce overfitting.\n",
    "\n",
    "Example:\n",
    "Consider a dataset where you are predicting house prices based on features like square footage, number of bedrooms, age of the house, proximity to the city center, and many others. If you use a linear regression model without regularization, the model might fit the training data very well, including noise, leading to poor performance on new, unseen data.\n",
    "\n",
    "However, if you apply Ridge regression, the model will still consider all features but with reduced coefficients. This can prevent overfitting by not allowing the model to overly weigh any single feature, especially those that might just be capturing noise in the training set.\n",
    "\n",
    "In the case of Lasso regression, it might completely eliminate the impact of less important features (by setting their coefficients to zero), like features that barely influence house prices or that are redundant. This simplifies the model and helps in focusing on the truly relevant features.\n",
    "\n",
    "Elastic Net will combine these approaches, potentially offering a balance that is useful if there are many correlated features or when you want to mix feature elimination with coefficient reduction.\n",
    "\n",
    "In conclusion, regularized linear models are essential for building models that not only perform well on training data but also generalize well to new data. By balancing model complexity and feature selection, they help create more robust models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
