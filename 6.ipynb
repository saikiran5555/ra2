{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b7b517a",
   "metadata": {},
   "source": [
    "Lasso regularization, also known as L1 regularization, is a technique used in regression models (like linear regression) to prevent overfitting and to handle multicollinearity. It does this by adding a penalty term to the cost function used in the regression model. The concept of Lasso regularization is closely related to Ridge regularization (L2 regularization), but there are key differences between the two.\n",
    "\n",
    "Lasso Regularization (L1):\n",
    "Penalty Term:\n",
    "The Lasso regularization adds a penalty equal to the absolute value of the magnitude of coefficients. The objective function becomes:\n",
    "Minimize \n",
    "(\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "^\n",
    "�\n",
    ")\n",
    "2\n",
    "+\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∣\n",
    "�\n",
    "�\n",
    "∣\n",
    ")\n",
    "Minimize (∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " +λ∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " ∣β \n",
    "j\n",
    "​\n",
    " ∣)\n",
    "where \n",
    "�\n",
    "^\n",
    "�\n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    "  is the predicted value, \n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    "  is the actual value, \n",
    "�\n",
    "�\n",
    "β \n",
    "j\n",
    "​\n",
    "  are the regression coefficients, \n",
    "�\n",
    "p is the number of predictors, and \n",
    "�\n",
    "λ is the regularization parameter.\n",
    "\n",
    "Feature Selection:\n",
    "Lasso has the ability to shrink coefficients to zero, effectively performing feature selection. This results in models that are simpler and more interpretable.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "When you have a large number of features and expect only a few of them to be important.\n",
    "When interpretability is a concern (since it provides simpler models).\n",
    "When feature selection is beneficial.\n",
    "Ridge Regularization (L2):\n",
    "Penalty Term:\n",
    "Ridge adds a penalty equal to the square of the magnitude of coefficients. The objective function is:\n",
    "Minimize \n",
    "(\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "^\n",
    "�\n",
    ")\n",
    "2\n",
    "+\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "�\n",
    "�\n",
    "2\n",
    ")\n",
    "Minimize (∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " +λ∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " β \n",
    "j\n",
    "2\n",
    "​\n",
    " )\n",
    "\n",
    "Coefficient Shrinkage:\n",
    "Ridge shrinks the coefficients but does not set them to zero, meaning that no features are ever completely eliminated from the model.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "When you have multicollinearity in your model.\n",
    "When you have more predictors than observations.\n",
    "To prevent overfitting while including all features in the model.\n",
    "Differences:\n",
    "Penalty Structure:\n",
    "\n",
    "Lasso uses an absolute value penalty (L1), leading to zero coefficients for certain variables.\n",
    "Ridge uses a squared penalty (L2), which decreases the size of coefficients but keeps them in the model.\n",
    "Feature Selection:\n",
    "\n",
    "Lasso can automatically perform feature selection.\n",
    "Ridge includes all features but with reduced magnitudes.\n",
    "Behavior with Correlated Features:\n",
    "\n",
    "Lasso might arbitrarily select one feature among the correlated ones.\n",
    "Ridge tends to shrink correlated features more uniformly.\n",
    "Model Complexity:\n",
    "\n",
    "Lasso can give simpler models with fewer parameters.\n",
    "Ridge tends to give more complex models that include all input features.\n",
    "When to Use Lasso Over Ridge:\n",
    "Use Lasso when you have a large number of features and believe that only a few of them are actually important, as Lasso will help in identifying these significant features.\n",
    "When model interpretability is important, and you want a model that is easier to understand, as Lasso will provide a simpler model with fewer variables.\n",
    "In cases where feature selection is crucial, especially if you need to reduce the dimensionality of your data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
